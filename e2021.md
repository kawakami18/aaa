# 第1章：線形代数

・行列に対する学問

### (1) 固有値・固有ベクトルの求め方を確認する。



𝐴𝑥⃗ =𝜆𝑥⃗ (𝐴−𝜆𝐼)𝑥⃗ =0⃗ 𝑥⃗ ≠0⃗ より





|𝐴−𝜆𝐼|=0∣∣∣1−𝜆243−𝜆∣∣∣=0(1−𝜆)(3−𝜆)−4·2=0𝜆=5 𝑜𝑟−1





(1243)(𝑥1𝑥2)=5(𝑥1𝑥2)よって𝑥1=𝑥2(1243)(𝑥1𝑥2)=−1(𝑥1𝑥2)よって𝑥1=−2𝑥2

したがって
𝜆=5のとき𝑥⃗ =(11)の定数倍𝜆=−1のとき𝑥⃗ =(2−1)の定数倍



### (2) 固有値分解について理解を深める。

・固有値分解をすることで分類できることや、似た特徴を見つけ出すことができる。

### (3) 特異値・特異ベクトルの概要を知る。

・特異値は1つに求まるが、特異ベクトルは複数あり、そのうちの1つが求まる。

### (4) 特異値分解の概要を知る。

・特異値分解は特殊な単位ベクトル、行列が長方形の型をしたものを計算する。 行列のままでは解が求まらないので、元の行列と、その行列を転置したものをかけ合わせることで正方形の行列が求まる。これ以降は固有値分解をする作業と一緒。 ・固有値分解、特異値分解をした画像データの行列から成分の小さい部分を取り除くことでデータ数を少なくすることができる。要するに画像圧縮ができる。

## 演習問題

問２.２.1 AB

𝐴=(2411),𝐵=(1331)



Aの1行目とBの1列目をかけ合わせる



2×1＋1×3＝5



Aの1行目とBの2列目をかけ合わせる



2×3＋1×1＝7



Aの2行目とBの1列目をかけ合わせる



4×1＋1×3＝7



Aの2行目とBの2列目をかけ合わせる



4×3＋1×1＝13



よって解が

𝐴𝐵=(57713)

となる。



# 第2章：確立・統計

### (1) 条件付き確率について理解を深める。

ある事象Bが与えられた下で、Aとなる確率

例　雨が降っている条件下で交通事故に遭う確率

雨が降っている条件下ということなので、この状況を100％と考える。 なので雨が降っていないことは考えなくて良い。 条件付き確率と同時確率をしっかりと区別して事象を捉えること。

### (2) ベイズ則の概要を知る。

一般的に事象Aと事象Bに対して

P(A)P(B|A) = P(B)P(A)　の数式になり、

A事象とB事象の集合 = B事象とA事象の集合　は同じことを表す。さらに、 A事象の起こる確率は、A事象という条件下で起こるB事象 = B事象の起こる確率は、B事象という条件下で起こるA事象も同じである。このように条件を入れ替えて求めることができることをベイズ則という。

### (3) 期待値・分散の求め方を確認する。

期待値は、概ね平均の値である。

詳しく説明すると、その分布における確立変数の平均の値、若しくは

ありえそうな値である。

事象X、確率変数f(X)、確率P(X)　とする。



期待値𝐸(𝑓)=∑𝑘=1𝑛𝑃(𝑋=𝑥𝑘)𝑓(𝑋=𝑥𝑘)

連続する値の場合





期待値𝐸(𝑓)=∫𝑃(𝑋=𝑥)𝑓(𝑋=𝑥)𝑑𝑥



分散は、データの散らばり具合のこと。

データの各々の値が、期待値からどれだけズレているのか平均したもの。



分散𝑉𝑎𝑟(𝑓)=𝐸((𝑓(𝑋=𝑥)−𝐸(𝑓))2)=𝐸(𝑓2(𝑋=𝑥))−(𝐸(𝑓))2



### (4) 様々な確率分布の概要を知る。

ベルヌーイ分布 ・コイントスのイメージ ・表と裏で出る割合が等しくなくても扱える



𝑃(𝑥|𝜇)=𝜇𝑥(1−𝜇)1−𝑥



マルチヌーイ（カテゴリカル）分布 ・サイコロを転がすイメージ ・各面の出る割合が等しくなくても扱える

二項分布 ・ベルヌーイ分布の多試行版



𝑃(𝑥|𝜆,𝑛)=𝑛!𝑥!(𝑛−𝑥)!𝜆𝑥(1−𝜆)𝑛−𝑥



ガウス分布 ・釣鐘型の連続分布



𝑁(𝑥;𝜇,𝜎2)=12𝜋𝜎2‾‾‾‾‾‾√𝑒𝑥𝑝(−12𝜎2(𝑥−𝜇)2)



## 演習問題　５章　条件付き確率　問5.1

洗濯物を干していた日という条件下で、雨が降ってきた日の発生する確率

条件付き確率



𝑃(雨が降ってきた日は12日|洗濯物を干していた日は60日）=



1260=15



洗濯物を干していてかつ雨が降ってきた日の発生する確率

同時確率



𝑃(雨が降ってきた日かつ洗濯物を干していた日は12日÷ある年の日数は365日）



## 第3章：情報理論

### (1) 自己情報量・シャノンエントロピーの定義を確認する。

自己情報量

・対数の底が2のとき、単位はbit



𝐼(𝑥)=−𝑙𝑜𝑔2(𝑃(𝑥))=𝑙𝑜𝑔2(𝑊(𝑥))



・対数の底がネイピアのeのとき、単位はnat



𝐼(𝑥)=−𝑙𝑜𝑔𝑒(𝑃(𝑥))=𝑙𝑜𝑔𝑒(𝑊(𝑥))





𝑙𝑜𝑔を用いた数式が現れたら情報量に関することだと意識するとよい。



シャノンエントロピー

・自己情報量の期待値、情報の珍しさの平均

・微分エントロピーとも呼ばれるが、微分しているわけではない。



𝐻(𝑥)=𝐸(𝐼(𝑥))=−𝐸(𝑙𝑜𝑔(𝑃(𝑥)))=−∑(𝑃(𝑥)𝑙𝑜𝑔(𝑃(𝑥)))



### (2) KLダイバージェンス・交差エントロピーの概要を知る。

カルバック・ライブラー　ダイバージェンス

・同じ事象・確率変数における異なる確率分布P,Qの違いを表す



𝐷𝐾𝐿(𝑃||𝑄)=𝔼𝑋〜𝑃[𝑙𝑜𝑔𝑃(𝑥)𝑄(𝑥)]=𝔼𝑥〜𝑃[𝑙𝑜𝑔𝑃(𝑥)−𝑙𝑜𝑔𝑄(𝑥)]



上記の式を書き換える



𝐼(𝑄(𝑥))−𝐼(𝑃(𝑥))=(−𝑙𝑜𝑔(𝑄(𝑥)))−(−𝑙𝑜𝑔(𝑃(𝑥)))=𝑙𝑜𝑔𝑃(𝑥)𝑄(𝑥)





𝐷𝐾𝐿(𝑃||𝑄)=∑𝑥𝑃(𝑥)(−𝑙𝑜𝑔(𝑄(𝑥)))−(−𝑙𝑜𝑔(𝑃(𝑥)))=∑𝑥𝑃(𝑥)𝑙𝑜𝑔𝑃(𝑥)𝑄(𝑥)



・シャノンエントロピーの形と似ているのでKLDと関係していると考えられる。

・元々考えられていた分布Qと実際の分布Pを比較し、どれだけ違いがあるか、距離のようなもので例えるならば、どれだけ近いのか、遠いのかを調べるときにKLDを用いる。

交差エントロピー

・KLダイバージェンスの一部分を取り出したもの

・Qについての自己情報量をPの分布で平均している

KLDとの関係



𝐷𝐾𝐿(𝑃||𝑄)=∑𝑥𝑃(𝑥)(−𝑙𝑜𝑔(𝑄(𝑥)))−(−𝑙𝑜𝑔(𝑃(𝑥)))





上記式の𝑃(𝑥)(−𝑙𝑜𝑔(𝑄(𝑥))に注目して考える。





𝐻(𝑃,𝑄)=𝐻(𝑃)+𝐷𝐾𝐿(𝑃||𝑄)





𝐻(𝑃,𝑄)=−𝔼𝑥〜𝑃𝑙𝑜𝑔𝑄(𝑥)=−∑𝑥𝑃(𝑥)𝑙𝑜𝑔𝑄(𝑥)



KDLから必要なものだけを取り出した式にり、効率的に解を求められる。

## 損失関数に用いる交差エントロピー

真の確率分布　P 推定した確率分布　Q 互いの確率分布が似ていると誤差数値は小さくなり、確率分布が似ていないと誤差数値は大きくなる。損失関数に確率的勾配降下法を用いる場合、交差エントロピーで誤差を求めることが多い。交差エントロピーで学習させた場合、出力されるデータと教師データの誤差が大きいほど損失関数の変動が大きくなり、よって学習速度は速くなる。